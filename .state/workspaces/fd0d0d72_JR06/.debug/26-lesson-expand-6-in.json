{
  "schema_version": "context_packet_v1",
  "packet_id": "22dbcf23-5dfb-43ea-91c9-2f89b3dfd6ef",
  "timestamp_utc": "2026-02-25T07:17:20.064Z",
  "role": "lesson-expand",
  "task_type": "lesson_expand",
  "learner_profile": {
    "masteryLevel": 0
  },
  "curriculum_context": {
    "node_id": "JR06",
    "depth_target": "D2"
  },
  "misconception_context": {
    "top_tags": []
  },
  "attempt_context": null,
  "evidence_context": null,
  "policy_context": {
    "allow_reveal": false
  },
  "memory_context": {},
  "scaffold_context": {
    "scaffold_id": "scaffold-JR06-001",
    "exercise_description": "Implement and validate a JS runtime structured-clone subsystem in C that serializes arbitrary worker messages, manages transferables safely, and exposes reliable postMessage IPC hooks.",
    "lesson_plan": {
      "section_intents": [
        "Motivate why structured clone powers high-throughput postMessage between Worker threads and overcomes JSON serialization limits in JS runtimes.",
        "Lay out the core structured clone algorithm phases (type tagging, traversal stack, transfer list validation) within the engine’s C/C++ subsystems.",
        "Explain lifecycle tracking for transferables—ArrayBuffer, SharedArrayBuffer, MessagePort—and how ownership moves between workers without data races.",
        "Worked example: step-by-step cloning of a nested object graph containing TypedArrays, highlighting stack operations and buffer duplication.",
        "Worked example: transferring a MessagePort plus SharedArrayBuffer between workers, detailing the syscall/IPC hooks and state transitions.",
        "Break down the serialized format layout (tags, lengths, handles) and differences for SharedArrayBuffer versus regular buffers.",
        "Compare structured clone semantics to JSON.stringify and custom serializers, focusing on deep copy vs move semantics, fidelity, and performance cost.",
        "Expose common pitfalls: non-cloneable objects, detached buffers, cyclic prototypes, and undefined behaviour when engine hooks skip slot sealing.",
        "Detail resource-leak and signal-safety risks when cloning is interrupted (worker termination, cancellation) and mitigation strategies.",
        "Bridge to next topic by linking structured clone constraints with upcoming transferable streams and zero-copy IPC scheduling."
      ]
    },
    "starter_plan": {
      "file_intents": [
        "src/structured_clone_context.c: declare stubs sc_context_init, sc_context_register_transferable, sc_context_free for lifecycle and registry management (3 stubs).",
        "src/structured_clone_encoder.c: declare stubs sc_encode_value, sc_encode_arraybuffer, sc_encode_shared_arraybuffer, sc_encode_postmessage_payload for serialization paths (4 stubs).",
        "src/structured_clone_executor.c: declare stubs sc_dispatch_to_worker, sc_transfer_ports, sc_commit_transfer_list for worker dispatch and transfer finalization (3 stubs)."
      ]
    },
    "test_plan": {
      "case_intents": [
        "Assert sc_context_init zeroes the arena, allocates traversal stacks, and returns 0 on success or -ENOMEM on failure (UB risk if fields left uninitialized).",
        "Assert sc_context_register_transferable records buffer handles exactly once and returns -EEXIST on duplicates (UB risk: dangling pointers if stub empty).",
        "Assert sc_context_free releases all arenas, detaches registered transferables, and is idempotent (UB risk: leaks + double-free).",
        "Assert sc_encode_value walks the graph depth-first, preserving object identity table and rejecting unsupported tags with -EINVAL.",
        "Assert sc_encode_arraybuffer copies bytes into the clone buffer and marks source as detached on transfer (UB risk: host sees stale pointer).",
        "Assert sc_encode_shared_arraybuffer emits handle IDs without copying and refuses transfer when SAB lacks atomics flag (UB risk: concurrent readers on freed memory).",
        "Assert sc_encode_postmessage_payload assembles header + payload vectors respecting max size limits and surfaces -EMSGSIZE on overflow.",
        "Assert sc_dispatch_to_worker posts the serialized payload to the target worker queue and signals readiness via eventfds.",
        "Assert sc_transfer_ports moves MessagePort ownership, closes sender entanglement, and surfaces -EBUSY if port already in transfer list (UB risk: double-close).",
        "Assert sc_commit_transfer_list finalizes ownership (detaches buffers, updates handles) and rolls back on mid-commit failure."
      ]
    }
  },
  "prior_loop_sections": [
    {
      "schema_version": "starter_section_v1",
      "role": "starter-expand",
      "section_id": "structured-clone-context-c",
      "type": "code",
      "file_path": "structured_clone_context.c",
      "content": "#include \"structured_clone_context.h\"\n#include <stddef.h>\n\nint sc_context_init(struct sc_context *ctx, size_t arena_bytes)\n{\n    (void)ctx;\n    (void)arena_bytes;\n\n    /* Initialize the structured clone arena and traversal stacks so the learner must zero buffers, allocate traversal storage, and propagate -ENOMEM style failures. */\n    return 0;\n}\n\nint sc_context_register_transferable(struct sc_context *ctx,\n                                     const struct sc_transferable_desc *desc)\n{\n    (void)ctx;\n    (void)desc;\n\n    /* Track a transferable in the context registry so the learner must detect duplicates, record handle metadata, and surface -EEXIST when the same handle is registered twice. */\n    return 0;\n}\n\nint sc_context_free(struct sc_context *ctx)\n{\n    (void)ctx;\n\n    /* Release the structured clone context so the learner must walk registered transferables, detach buffers, free arenas, and keep the routine idempotent. */\n    return 0;\n}\n",
      "is_complete": false,
      "next_focus": "structured_clone_context.h"
    },
    {
      "schema_version": "starter_section_v1",
      "role": "starter-expand",
      "section_id": "structured-clone-context-h",
      "type": "code",
      "file_path": "structured_clone_context.h",
      "content": "#ifndef STRUCTURED_CLONE_CONTEXT_H\n#define STRUCTURED_CLONE_CONTEXT_H\n\n#include <stddef.h>\n\nstruct sc_transferable_desc {\n    void *handle;\n    size_t length;\n    unsigned int flags;\n};\n\nstruct sc_context {\n    void *arena_base;\n    size_t arena_size;\n    size_t arena_used;\n    unsigned int transfer_count;\n    void *transfer_entries;\n};\n\n/* Initialize the structured clone arena and traversal stacks so the learner must zero buffers, allocate traversal storage, and propagate -ENOMEM style failures. */\nint sc_context_init(struct sc_context *ctx, size_t arena_bytes);\n\n/* Track a transferable in the context registry so the learner must detect duplicates, record handle metadata, and surface -EEXIST when the same handle is registered twice. */\nint sc_context_register_transferable(struct sc_context *ctx,\n                                     const struct sc_transferable_desc *desc);\n\n/* Release the structured clone context so the learner must walk registered transferables, detach buffers, free arenas, and keep the routine idempotent. */\nint sc_context_free(struct sc_context *ctx);\n\n#endif /* STRUCTURED_CLONE_CONTEXT_H */\n",
      "is_complete": false,
      "next_focus": "structured_clone_encoder.c"
    },
    {
      "schema_version": "starter_section_v1",
      "role": "starter-expand",
      "section_id": "structured-clone-encoder-c",
      "type": "code",
      "file_path": "structured_clone_encoder.c",
      "content": "#include \"structured_clone_encoder.h\"\n\nint sc_encode_value(struct sc_context *ctx,\n                    const struct sc_value *root,\n                    struct sc_encoded_buffer *out)\n{\n    (void)ctx;\n    (void)root;\n    (void)out;\n\n    /* Encode an arbitrary worker message so the learner must walk the graph depth-first, attach type tags, and surface -EINVAL for unsupported constructs. */\n    return 0;\n}\n\nint sc_encode_arraybuffer(struct sc_context *ctx,\n                          const struct sc_arraybuffer_desc *src,\n                          struct sc_encoded_buffer *out)\n{\n    (void)ctx;\n    (void)src;\n    (void)out;\n\n    /* Clone an ArrayBuffer payload so the learner must copy bytes, mark detachment out of band, and flag -EMSGSIZE when the buffer exceeds limits. */\n    return 0;\n}\n\nint sc_encode_shared_arraybuffer(struct sc_context *ctx,\n                                 const struct sc_shared_arraybuffer_desc *src,\n                                 struct sc_encoded_buffer *out)\n{\n    (void)ctx;\n    (void)src;\n    (void)out;\n\n    /* Serialize a SharedArrayBuffer handle so the learner must emit handle IDs, check atomics capability, and reject illegal transfers with -EPERM. */\n    return 0;\n}\n\nint sc_encode_postmessage_payload(struct sc_context *ctx,\n                                  const struct sc_message_record *record,\n                                  struct sc_encoded_buffer *out)\n{\n    (void)ctx;\n    (void)record;\n    (void)out;\n\n    /* Assemble the final postMessage payload so the learner must stitch headers, body vectors, and enforce maximum IPC frame sizes before enqueueing. */\n    return 0;\n}\n",
      "is_complete": false,
      "next_focus": "structured_clone_encoder.h"
    },
    {
      "schema_version": "starter_section_v1",
      "role": "starter-expand",
      "section_id": "structured-clone-encoder-h",
      "type": "code",
      "file_path": "structured_clone_encoder.h",
      "content": "#ifndef STRUCTURED_CLONE_ENCODER_H\n#define STRUCTURED_CLONE_ENCODER_H\n\n#include <stddef.h>\n#include \"structured_clone_context.h\"\n\nstruct sc_encoded_buffer {\n    void *data;\n    size_t length;\n    size_t capacity;\n};\n\nstruct sc_value {\n    const void *opaque;\n    unsigned int tag;\n};\n\nstruct sc_arraybuffer_desc {\n    const void *data;\n    size_t length;\n    unsigned int flags;\n};\n\nstruct sc_shared_arraybuffer_desc {\n    int handle;\n    size_t length;\n    unsigned int flags;\n};\n\nstruct sc_message_record {\n    const struct sc_value *root;\n    const struct sc_transferable_desc *transfer_list;\n    size_t transfer_count;\n};\n\n/* Encode an arbitrary worker message so the learner must walk the graph depth-first, attach type tags, and surface -EINVAL for unsupported constructs. */\nint sc_encode_value(struct sc_context *ctx,\n                    const struct sc_value *root,\n                    struct sc_encoded_buffer *out);\n\n/* Clone an ArrayBuffer payload so the learner must copy bytes, mark detachment out of band, and flag -EMSGSIZE when the buffer exceeds limits. */\nint sc_encode_arraybuffer(struct sc_context *ctx,\n                          const struct sc_arraybuffer_desc *src,\n                          struct sc_encoded_buffer *out);\n\n/* Serialize a SharedArrayBuffer handle so the learner must emit handle IDs, check atomics capability, and reject illegal transfers with -EPERM. */\nint sc_encode_shared_arraybuffer(struct sc_context *ctx,\n                                 const struct sc_shared_arraybuffer_desc *src,\n                                 struct sc_encoded_buffer *out);\n\n/* Assemble the final postMessage payload so the learner must stitch headers, body vectors, and enforce maximum IPC frame sizes before enqueueing. */\nint sc_encode_postmessage_payload(struct sc_context *ctx,\n                                  const struct sc_message_record *record,\n                                  struct sc_encoded_buffer *out);\n\n#endif /* STRUCTURED_CLONE_ENCODER_H */\n",
      "is_complete": false,
      "next_focus": "structured_clone_executor.c"
    },
    {
      "schema_version": "starter_section_v1",
      "role": "starter-expand",
      "section_id": "structured-clone-executor-c",
      "type": "code",
      "file_path": "structured_clone_executor.c",
      "content": "#include \"structured_clone_executor.h\"\n\nint sc_dispatch_to_worker(struct sc_context *ctx,\n                          struct sc_worker_endpoint *target,\n                          const struct sc_encoded_buffer *payload)\n{\n    (void)ctx;\n    (void)target;\n    (void)payload;\n\n    /* Dispatch the encoded payload to a worker so the learner must enqueue IPC frames, signal readiness via eventfd-like handles, and surface transport failures from the runtime. */\n    return 0;\n}\n\nint sc_transfer_ports(struct sc_context *ctx,\n                      struct sc_transferable_desc *ports,\n                      size_t port_count)\n{\n    (void)ctx;\n    (void)ports;\n    (void)port_count;\n\n    /* Move MessagePort ownership so the learner must validate entanglement pairs, refuse duplicates with -EBUSY, and close the sender endpoint once transfer is committed. */\n    return 0;\n}\n\nint sc_commit_transfer_list(struct sc_context *ctx,\n                            const struct sc_message_record *record)\n{\n    (void)ctx;\n    (void)record;\n\n    /* Finalize the transfer list so the learner must detach buffers, update handle tables atomically, and roll back the entire commit if any resource finalization fails. */\n    return 0;\n}\n",
      "is_complete": false,
      "next_focus": "structured_clone_executor.h"
    },
    {
      "schema_version": "starter_section_v1",
      "role": "starter-expand",
      "section_id": "structured-clone-executor-h",
      "type": "code",
      "file_path": "structured_clone_executor.h",
      "content": "#ifndef STRUCTURED_CLONE_EXECUTOR_H\n#define STRUCTURED_CLONE_EXECUTOR_H\n\n#include <stddef.h>\n#include \"structured_clone_context.h\"\n#include \"structured_clone_encoder.h\"\n\nstruct sc_worker_endpoint {\n    int queue_fd;\n    int signal_fd;\n    unsigned int flags;\n};\n\n/* Dispatch the encoded payload to a worker so the learner must marshal the IPC frame, signal readiness to the worker loop, and propagate transport errors from the runtime. */\nint sc_dispatch_to_worker(struct sc_context *ctx,\n                          struct sc_worker_endpoint *target,\n                          const struct sc_encoded_buffer *payload);\n\n/* Transfer ownership of MessagePort instances so the learner must validate entanglement pairs, reject duplicates with -EBUSY, and close sender endpoints when transfers succeed. */\nint sc_transfer_ports(struct sc_context *ctx,\n                      struct sc_transferable_desc *ports,\n                      size_t port_count);\n\n/* Commit the transfer list so the learner must detach buffers, update handle tables atomically, and roll back state when any finalization step fails mid-commit. */\nint sc_commit_transfer_list(struct sc_context *ctx,\n                            const struct sc_message_record *record);\n\n#endif /* STRUCTURED_CLONE_EXECUTOR_H */\n",
      "is_complete": true,
      "next_focus": ""
    },
    {
      "schema_version": "test_section_v1",
      "role": "test-expand",
      "section_id": "structured-clone-tests",
      "type": "code",
      "file_path": "test_structured_clone.c",
      "content": "#include \"test.h\"\n#include \"structured_clone_context.h\"\n#include \"structured_clone_encoder.h\"\n#include \"structured_clone_executor.h\"\n#include <errno.h>\n#include <limits.h>\n#include <stdbool.h>\n#include <stdint.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/types.h>\n#include <unistd.h>\n\n#define TRANSFER_FLAG_REGISTERED 0x1u\n#define TRANSFER_FLAG_DETACHED   0x2u\n#define TRANSFER_FLAG_IDENTITY   0x4u\n#define TRANSFER_FLAG_IN_PORT    0x8u\n#define TRANSFER_FLAG_COMMITTED  0x10u\n#define TRANSFER_FLAG_FAIL_COMMIT 0x20u\n\n#define SAB_FLAG_HAS_ATOMICS 0x1u\n\n#define TAG_PRIMITIVE 0x10u\n#define TAG_SEQUENCE 0x11u\n#define TAG_REFERENCE 0x12u\n#define TAG_ARRAYBUFFER 0x20u\n#define TAG_SHARED_ARRAYBUFFER 0x21u\n#define TAG_UNSUPPORTED 0xFFFFu\n\nstruct sc_primitive_value {\n    const uint8_t *bytes;\n    size_t length;\n};\n\nstruct sc_sequence_value {\n    const struct sc_value *const *children;\n    size_t child_count;\n};\n\nstruct test_port {\n    bool closed;\n    int id;\n};\n\nstruct commit_payload {\n    bool committed;\n};\n\nstatic uint32_t read_u32(const uint8_t *base, size_t offset)\n{\n    uint32_t val = 0;\n    memcpy(&val, base + offset, sizeof(val));\n    return val;\n}\n\nstatic void assert_transfer_entry(const struct sc_context *ctx,\n                                  size_t idx,\n                                  const void *handle,\n                                  size_t length,\n                                  unsigned int flag_mask)\n{\n    TEST_ASSERT(ctx->transfer_entries != NULL);\n    const struct sc_transferable_desc *entries =\n        (const struct sc_transferable_desc *)ctx->transfer_entries;\n    TEST_ASSERT(idx < ctx->transfer_count);\n    TEST_ASSERT_EQ(handle, entries[idx].handle);\n    TEST_ASSERT_EQ(length, entries[idx].length);\n    TEST_ASSERT((entries[idx].flags & flag_mask) == flag_mask);\n}\n\nstatic void test_sc_context_init_zeroes_arena(void)\n{\n    struct sc_context ctx = {\n        .arena_base = (void *)0xDEADBEEF,\n        .arena_size = 123,\n        .arena_used = 7,\n        .transfer_count = 3,\n        .transfer_entries = (void *)0xCAFEBABE\n    };\n\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 256));\n    TEST_ASSERT(ctx.arena_base != NULL);\n    TEST_ASSERT_EQ(256u, ctx.arena_size);\n    TEST_ASSERT_EQ(0u, ctx.arena_used);\n    TEST_ASSERT_EQ(0u, ctx.transfer_count);\n    TEST_ASSERT(ctx.transfer_entries != NULL);\n\n    const uint8_t *arena = (const uint8_t *)ctx.arena_base;\n    for (size_t i = 0; i < 32; ++i) {\n        TEST_ASSERT_EQ(0u, arena[i]);\n    }\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_context_init_enomem_preserves_state(void)\n{\n    struct sc_context ctx = {\n        .arena_base = (void *)0x1,\n        .arena_size = 32,\n        .arena_used = 4,\n        .transfer_count = 2,\n        .transfer_entries = (void *)0x2\n    };\n\n    int rc = sc_context_init(&ctx, SIZE_MAX);\n    TEST_ASSERT_EQ(-ENOMEM, rc);\n    TEST_ASSERT_EQ((void *)0x1, ctx.arena_base);\n    TEST_ASSERT_EQ(32u, ctx.arena_size);\n    TEST_ASSERT_EQ(4u, ctx.arena_used);\n    TEST_ASSERT_EQ(2u, ctx.transfer_count);\n    TEST_ASSERT_EQ((void *)0x2, ctx.transfer_entries);\n}\n\nstatic void test_sc_context_register_transferable_records_handle(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 512));\n\n    uint8_t buffer[16] = {0};\n    struct sc_transferable_desc desc = {\n        .handle = buffer,\n        .length = sizeof(buffer),\n        .flags = 0\n    };\n\n    TEST_ASSERT_EQ(0, sc_context_register_transferable(&ctx, &desc));\n    TEST_ASSERT_EQ(1u, ctx.transfer_count);\n    assert_transfer_entry(&ctx, 0, buffer, sizeof(buffer), TRANSFER_FLAG_REGISTERED);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_context_register_transferable_duplicate_returns_eexist(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 512));\n\n    uint8_t buffer[8] = {0};\n    struct sc_transferable_desc desc = {\n        .handle = buffer,\n        .length = sizeof(buffer),\n        .flags = 0\n    };\n\n    TEST_ASSERT_EQ(0, sc_context_register_transferable(&ctx, &desc));\n    int rc = sc_context_register_transferable(&ctx, &desc);\n    TEST_ASSERT_EQ(-EEXIST, rc);\n    TEST_ASSERT_EQ(1u, ctx.transfer_count);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_context_free_idempotent(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 128));\n\n    uint8_t buffer[4] = {0};\n    struct sc_transferable_desc desc = {\n        .handle = buffer,\n        .length = sizeof(buffer),\n        .flags = 0\n    };\n    TEST_ASSERT_EQ(0, sc_context_register_transferable(&ctx, &desc));\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n    TEST_ASSERT_EQ(NULL, ctx.arena_base);\n    TEST_ASSERT_EQ(0u, ctx.arena_size);\n    TEST_ASSERT_EQ(0u, ctx.arena_used);\n    TEST_ASSERT_EQ(0u, ctx.transfer_count);\n    TEST_ASSERT_EQ(NULL, ctx.transfer_entries);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_encode_value_depth_first_identity(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 512));\n\n    uint8_t encoded[256] = {0};\n    struct sc_encoded_buffer out = {\n        .data = encoded,\n        .length = 0,\n        .capacity = sizeof(encoded)\n    };\n\n    static const uint8_t left_bytes[] = { 'L' };\n    static const uint8_t right_bytes[] = { 'R', 'T' };\n    struct sc_primitive_value left_payload = { left_bytes, sizeof(left_bytes) };\n    struct sc_primitive_value right_payload = { right_bytes, sizeof(right_bytes) };\n    struct sc_value left = { &left_payload, TAG_PRIMITIVE };\n    struct sc_value right = { &right_payload, TAG_PRIMITIVE };\n    const struct sc_value *children[] = { &left, &right, &left };\n    struct sc_sequence_value seq = { children, 3 };\n    struct sc_value root = { &seq, TAG_SEQUENCE };\n\n    TEST_ASSERT_EQ(0, sc_encode_value(&ctx, &root, &out));\n    TEST_ASSERT(out.length > 0);\n    TEST_ASSERT_EQ(3u, ctx.transfer_count);\n    assert_transfer_entry(&ctx, 0, &root, 0, TRANSFER_FLAG_IDENTITY);\n    assert_transfer_entry(&ctx, 1, &left, 0, TRANSFER_FLAG_IDENTITY);\n    assert_transfer_entry(&ctx, 2, &right, 0, TRANSFER_FLAG_IDENTITY);\n\n    const uint8_t *bytes = encoded;\n    size_t offset = 0;\n    TEST_ASSERT_EQ(TAG_SEQUENCE, read_u32(bytes, offset));\n    offset += 4;\n    uint32_t seq_len = read_u32(bytes, offset);\n    offset += 4;\n    size_t seq_end = offset + seq_len;\n    TEST_ASSERT(seq_end <= out.length);\n\n    uint32_t child_count = read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT_EQ(3u, child_count);\n\n    TEST_ASSERT_EQ(TAG_PRIMITIVE, read_u32(bytes, offset));\n    offset += 4;\n    uint32_t child_len = read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT(child_len >= 4);\n    uint32_t payload_len = read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT_EQ(left_payload.length, payload_len);\n    TEST_ASSERT_EQ(left_bytes[0], bytes[offset]);\n    offset += payload_len;\n\n    TEST_ASSERT_EQ(TAG_PRIMITIVE, read_u32(bytes, offset));\n    offset += 4;\n    child_len = read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT(child_len >= 4);\n    payload_len = read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT_EQ(right_payload.length, payload_len);\n    TEST_ASSERT_EQ(right_bytes[0], bytes[offset]);\n    TEST_ASSERT_EQ(right_bytes[1], bytes[offset + 1]);\n    offset += payload_len;\n\n    TEST_ASSERT_EQ(TAG_REFERENCE, read_u32(bytes, offset));\n    offset += 4;\n    child_len = read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT_EQ(sizeof(uint32_t), child_len);\n    uint32_t ref_slot = read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT_EQ(1u, ref_slot);\n\n    TEST_ASSERT_EQ(seq_end, offset);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_encode_value_rejects_unknown_tag(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 128));\n\n    uint8_t encoded[32] = {0};\n    struct sc_encoded_buffer out = {\n        .data = encoded,\n        .length = 0,\n        .capacity = sizeof(encoded)\n    };\n    struct sc_value invalid = { NULL, TAG_UNSUPPORTED };\n\n    TEST_ASSERT_EQ(-EINVAL, sc_encode_value(&ctx, &invalid, &out));\n    TEST_ASSERT_EQ(0u, out.length);\n    TEST_ASSERT_EQ(0u, ctx.transfer_count);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_encode_arraybuffer_copies_payload_and_marks_detached(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 128));\n\n    uint8_t out_buf[128] = {0};\n    struct sc_encoded_buffer out = { out_buf, 0, sizeof(out_buf) };\n\n    uint8_t src_bytes[4] = { 1, 2, 3, 4 };\n    uint8_t expected[4];\n    memcpy(expected, src_bytes, sizeof(src_bytes));\n    struct sc_arraybuffer_desc src = { src_bytes, sizeof(src_bytes), 0 };\n\n    TEST_ASSERT_EQ(0, sc_encode_arraybuffer(&ctx, &src, &out));\n    TEST_ASSERT(out.length > 0);\n\n    const uint8_t *bytes = out_buf;\n    size_t offset = 0;\n    TEST_ASSERT_EQ(TAG_ARRAYBUFFER, read_u32(bytes, offset));\n    offset += 4;\n    uint32_t payload_len = read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT_EQ(sizeof(uint32_t) + sizeof(src_bytes), payload_len);\n    uint32_t byte_length = read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT_EQ(src.length, byte_length);\n    TEST_ASSERT_EQ(0, memcmp(bytes + offset, expected, sizeof(expected)));\n\n    assert_transfer_entry(&ctx, 0, src_bytes, sizeof(src_bytes), TRANSFER_FLAG_DETACHED);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_encode_arraybuffer_rejects_emsgsize(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 64));\n\n    uint8_t out_buf[8] = {0};\n    struct sc_encoded_buffer out = { out_buf, 0, sizeof(out_buf) };\n    uint8_t src_bytes[16] = {0};\n    struct sc_arraybuffer_desc src = { src_bytes, sizeof(src_bytes), 0 };\n\n    TEST_ASSERT_EQ(-EMSGSIZE, sc_encode_arraybuffer(&ctx, &src, &out));\n    TEST_ASSERT_EQ(0u, out.length);\n    TEST_ASSERT_EQ(0u, ctx.transfer_count);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_encode_shared_arraybuffer_handles_without_copy(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 128));\n\n    uint8_t out_buf[128] = {0};\n    struct sc_encoded_buffer out = { out_buf, 0, sizeof(out_buf) };\n\n    struct sc_shared_arraybuffer_desc sab = {\n        .handle = 42,\n        .length = 2048,\n        .flags = SAB_FLAG_HAS_ATOMICS\n    };\n\n    TEST_ASSERT_EQ(0, sc_encode_shared_arraybuffer(&ctx, &sab, &out));\n\n    const uint8_t *bytes = out_buf;\n    size_t offset = 0;\n    TEST_ASSERT_EQ(TAG_SHARED_ARRAYBUFFER, read_u32(bytes, offset));\n    offset += 4;\n    uint32_t payload_len = read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT_EQ(sizeof(int32_t) + sizeof(uint32_t) + sizeof(uint32_t), payload_len);\n    int32_t handle = (int32_t)read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT_EQ(sab.handle, handle);\n    uint32_t length = read_u32(bytes, offset);\n    offset += 4;\n    TEST_ASSERT_EQ(sab.length, length);\n\n    assert_transfer_entry(&ctx, 0, (void *)(intptr_t)sab.handle,\n                          sab.length, TRANSFER_FLAG_REGISTERED);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_encode_shared_arraybuffer_requires_atomics(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 64));\n\n    uint8_t out_buf[32] = {0};\n    struct sc_encoded_buffer out = { out_buf, 0, sizeof(out_buf) };\n    struct sc_shared_arraybuffer_desc sab = {\n        .handle = 7,\n        .length = 128,\n        .flags = 0\n    };\n\n    TEST_ASSERT_EQ(-EPERM, sc_encode_shared_arraybuffer(&ctx, &sab, &out));\n    TEST_ASSERT_EQ(0u, out.length);\n    TEST_ASSERT_EQ(0u, ctx.transfer_count);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_encode_postmessage_payload_builds_header(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 512));\n\n    uint8_t out_buf[256] = {0};\n    struct sc_encoded_buffer out = { out_buf, 0, sizeof(out_buf) };\n    static const uint8_t bytes[] = { 'o', 'k' };\n    struct sc_primitive_value prim = { bytes, sizeof(bytes) };\n    struct sc_value root = { &prim, TAG_PRIMITIVE };\n    struct sc_message_record record = {\n        .root = &root,\n        .transfer_list = NULL,\n        .transfer_count = 0\n    };\n\n    TEST_ASSERT_EQ(0, sc_encode_postmessage_payload(&ctx, &record, &out));\n    size_t header_len = sizeof(uint32_t) * 2;\n    TEST_ASSERT(out.length >= header_len);\n\n    const uint8_t *bytes_out = out_buf;\n    uint32_t payload_len = read_u32(bytes_out, 0);\n    uint32_t transfer_count = read_u32(bytes_out, sizeof(uint32_t));\n    TEST_ASSERT_EQ(record.transfer_count, transfer_count);\n    TEST_ASSERT_EQ(out.length - header_len, payload_len);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_encode_postmessage_payload_overflow_returns_emsgsize(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 128));\n\n    uint8_t out_buf[8] = {0};\n    struct sc_encoded_buffer out = { out_buf, 0, sizeof(out_buf) };\n    static const uint8_t bytes[] = { 1, 2, 3, 4, 5, 6 };\n    struct sc_primitive_value prim = { bytes, sizeof(bytes) };\n    struct sc_value root = { &prim, TAG_PRIMITIVE };\n    struct sc_message_record record = {\n        .root = &root,\n        .transfer_list = NULL,\n        .transfer_count = 0\n    };\n\n    TEST_ASSERT_EQ(-EMSGSIZE, sc_encode_postmessage_payload(&ctx, &record, &out));\n    TEST_ASSERT_EQ(0u, out.length);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_dispatch_to_worker_writes_and_signals(void)\n{\n    int queue_pipe[2];\n    int signal_pipe[2];\n    TEST_ASSERT_EQ(0, pipe(queue_pipe));\n    TEST_ASSERT_EQ(0, pipe(signal_pipe));\n\n    struct sc_worker_endpoint worker = {\n        .queue_fd = queue_pipe[1],\n        .signal_fd = signal_pipe[1],\n        .flags = 0\n    };\n\n    uint8_t payload_bytes[] = { 'i', 'p', 'c' };\n    struct sc_encoded_buffer payload = {\n        .data = payload_bytes,\n        .length = sizeof(payload_bytes),\n        .capacity = sizeof(payload_bytes)\n    };\n    struct sc_context ctx = {0};\n\n    TEST_ASSERT_EQ(0, sc_dispatch_to_worker(&ctx, &worker, &payload));\n\n    uint8_t queue_buf[8] = {0};\n    ssize_t n = read(queue_pipe[0], queue_buf, sizeof(queue_buf));\n    TEST_ASSERT_EQ((ssize_t)payload.length, n);\n    TEST_ASSERT_EQ(0, memcmp(queue_buf, payload_bytes, payload.length));\n\n    uint8_t signal_byte = 0;\n    TEST_ASSERT_EQ(1, read(signal_pipe[0], &signal_byte, 1));\n    TEST_ASSERT_EQ(1u, signal_byte);\n\n    close(queue_pipe[0]);\n    close(queue_pipe[1]);\n    close(signal_pipe[0]);\n    close(signal_pipe[1]);\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_transfer_ports_moves_handles_and_blocks_busy(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 256));\n\n    struct test_port port_state[2] = {\n        { .closed = false, .id = 1 },\n        { .closed = false, .id = 2 }\n    };\n\n    struct sc_transferable_desc ports[2] = {\n        { .handle = &port_state[0], .length = sizeof(struct test_port), .flags = 0 },\n        { .handle = &port_state[1], .length = sizeof(struct test_port), .flags = 0 }\n    };\n\n    TEST_ASSERT_EQ(0, sc_transfer_ports(&ctx, ports, 2));\n    TEST_ASSERT_EQ(NULL, ports[0].handle);\n    TEST_ASSERT_EQ(NULL, ports[1].handle);\n    TEST_ASSERT((ports[0].flags & TRANSFER_FLAG_IN_PORT) != 0);\n    TEST_ASSERT((ports[1].flags & TRANSFER_FLAG_IN_PORT) != 0);\n\n    struct sc_transferable_desc busy = {\n        .handle = &port_state[0],\n        .length = sizeof(struct test_port),\n        .flags = TRANSFER_FLAG_IN_PORT\n    };\n    TEST_ASSERT_EQ(-EBUSY, sc_transfer_ports(&ctx, &busy, 1));\n    TEST_ASSERT_EQ(&port_state[0], busy.handle);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_commit_transfer_list_finalizes_handles(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 256));\n\n    struct commit_payload payloads[2] = {0};\n    struct sc_transferable_desc transfers[2] = {\n        { .handle = &payloads[0], .length = sizeof(payloads[0]), .flags = TRANSFER_FLAG_IN_PORT },\n        { .handle = &payloads[1], .length = sizeof(payloads[1]), .flags = TRANSFER_FLAG_IN_PORT }\n    };\n    struct sc_message_record record = {\n        .root = NULL,\n        .transfer_list = transfers,\n        .transfer_count = 2\n    };\n\n    TEST_ASSERT_EQ(0, sc_commit_transfer_list(&ctx, &record));\n    TEST_ASSERT_EQ(NULL, transfers[0].handle);\n    TEST_ASSERT_EQ(NULL, transfers[1].handle);\n    TEST_ASSERT((transfers[0].flags & TRANSFER_FLAG_COMMITTED) != 0);\n    TEST_ASSERT((transfers[1].flags & TRANSFER_FLAG_COMMITTED) != 0);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nstatic void test_sc_commit_transfer_list_rolls_back_on_failure(void)\n{\n    struct sc_context ctx;\n    TEST_ASSERT_EQ(0, sc_context_init(&ctx, 256));\n\n    struct commit_payload payloads[2] = {0};\n    struct sc_transferable_desc transfers[2] = {\n        { .handle = &payloads[0], .length = sizeof(payloads[0]), .flags = TRANSFER_FLAG_IN_PORT },\n        { .handle = &payloads[1], .length = sizeof(payloads[1]),\n          .flags = TRANSFER_FLAG_IN_PORT | TRANSFER_FLAG_FAIL_COMMIT }\n    };\n    struct sc_message_record record = {\n        .root = NULL,\n        .transfer_list = transfers,\n        .transfer_count = 2\n    };\n\n    TEST_ASSERT_EQ(-EIO, sc_commit_transfer_list(&ctx, &record));\n    TEST_ASSERT_EQ(&payloads[0], transfers[0].handle);\n    TEST_ASSERT_EQ(&payloads[1], transfers[1].handle);\n    TEST_ASSERT((transfers[0].flags & TRANSFER_FLAG_COMMITTED) == 0);\n\n    TEST_ASSERT_EQ(0, sc_context_free(&ctx));\n}\n\nint main(void)\n{\n    RUN_TEST(test_sc_context_init_zeroes_arena);\n    RUN_TEST(test_sc_context_init_enomem_preserves_state);\n    RUN_TEST(test_sc_context_register_transferable_records_handle);\n    RUN_TEST(test_sc_context_register_transferable_duplicate_returns_eexist);\n    RUN_TEST(test_sc_context_free_idempotent);\n    RUN_TEST(test_sc_encode_value_depth_first_identity);\n    RUN_TEST(test_sc_encode_value_rejects_unknown_tag);\n    RUN_TEST(test_sc_encode_arraybuffer_copies_payload_and_marks_detached);\n    RUN_TEST(test_sc_encode_arraybuffer_rejects_emsgsize);\n    RUN_TEST(test_sc_encode_shared_arraybuffer_handles_without_copy);\n    RUN_TEST(test_sc_encode_shared_arraybuffer_requires_atomics);\n    RUN_TEST(test_sc_encode_postmessage_payload_builds_header);\n    RUN_TEST(test_sc_encode_postmessage_payload_overflow_returns_emsgsize);\n    RUN_TEST(test_sc_dispatch_to_worker_writes_and_signals);\n    RUN_TEST(test_sc_transfer_ports_moves_handles_and_blocks_busy);\n    RUN_TEST(test_sc_commit_transfer_list_finalizes_handles);\n    RUN_TEST(test_sc_commit_transfer_list_rolls_back_on_failure);\n    return TEST_SUMMARY();\n}\n",
      "is_complete": true,
      "next_focus": ""
    }
  ],
  "current_sections": [
    {
      "schema_version": "lesson_section_v1",
      "role": "lesson-expand",
      "section_id": "motivation-structured-clone",
      "type": "concept",
      "content": "Structured clone is the engine trick that lets postMessage behave like shared memory without the races: it lets you queue arbitrarily complex graphs while each Worker still believes it owns its own copy. JSON tops out at numbers, strings, and trees; any ArrayBuffer, TypedArray, Map, Set, or cyclic graph forces you to either flatten the data by hand or bounce through shared globals—both are bottlenecks once you need millions of messages per second. With structured clone, the runtime treats a message like a mini object file: typed tags, deterministic traversal order, and a transfer list that can move ownership of huge buffers rather than copying them. That is why browsers and JS shells rely on it to keep UI, network, and storage workers busy without blocking the main loop. In C you wire this up with a narrow API that JS shims call into, e.g.\n```c\nint postmessage_clone(struct sc_context *ctx, const struct sc_value *value,\n                      struct sc_message_record *record)\n{\n    if (sc_context_init(ctx, 64 * 1024))\n        return -1;\n    if (sc_encode_value(ctx, value, &record->encoded))\n        return sc_context_free(ctx);\n    return sc_encode_postmessage_payload(ctx, record, &record->encoded);\n}\n```\nHere the context becomes a scratch arena owned by the runtime, so any Worker can emit a high-volume stream of heterogeneous messages without allocating through the JS heap or stalling on JSON stringify/parse cycles.",
      "is_complete": false,
      "next_focus": "Lay out the core structured clone algorithm phases (type tagging, traversal stack, transfer list validation) within the engine’s C/C++ subsystems."
    },
    {
      "schema_version": "lesson_section_v1",
      "role": "lesson-expand",
      "section_id": "structured-clone-phases",
      "type": "concept",
      "content": "Inside a runtime the structured clone path is three interlocking loops: tag emission, traversal bookkeeping, and transferable vetting. Tagging is the lingua franca between engine and Worker—the encoder maps every JS shape to a compact enum plus length prefix so the decoder never has to second-guess byte boundaries. The traversal stack is a deterministic DFS queue that remembers every edge you have already walked, allowing identity-preserving back-references without recursion depth blowups. Finally, the transfer list is a guarded side channel: every ArrayBuffer, SharedArrayBuffer, or MessagePort is validated once, annotated with ownership bits, and either duplicated or moved before the payload leaves the thread. A minimal C skeleton looks like this:\n```c\nint sc_clone(struct sc_context *ctx, const struct sc_value *root,\n             struct sc_encoded_buffer *encoded,\n             struct sc_transferable_desc *xfer, size_t xfer_count)\n{\n    if (sc_context_init(ctx, 64 * 1024))\n        return -ENOMEM;\n    if (sc_encode_value(ctx, root, encoded))\n        return sc_context_free(ctx);\n    if (sc_commit_transfer_list(ctx, &(struct sc_message_record){\n            .root = root, .transfer_list = xfer, .transfer_count = xfer_count }))\n        return sc_context_free(ctx);\n    return 0;\n}\n```\nReal engines split those calls across subsystems but the flow is identical: type tags are written as you pop items off the traversal stack, every new pointer is registered into the identity table, and the transfer validator either decorates the encoded stream with handles (SAB, ports) or copies bytes (ArrayBuffers) before the executor posts the message.",
      "is_complete": false,
      "next_focus": "Explain lifecycle tracking for transferables—ArrayBuffer, SharedArrayBuffer, MessagePort—and how ownership moves between workers without data races."
    },
    {
      "schema_version": "lesson_section_v1",
      "role": "lesson-expand",
      "section_id": "transferable-lifecycle-tracking",
      "type": "concept",
      "content": "Transferables are a contract: once a buffer or port enters the transfer list, the sender must never touch it again, and the receiver must know whether it owns a byte copy or a handle. A structured clone context tracks this through three lifecycles. ArrayBuffer entries start as registered handles, get duplicated into the destination arena, and are marked detached so the JS engine reuses the backing store pointer slot but zeros its length. SharedArrayBuffer entries skip the copy; instead we log their OS handle plus atomic-capability flag and ensure the target worker increments the reference count before the source decrements. MessagePorts behave like duplex pipes: the sender marks the entangled endpoint as \"closing\", the executor enqueues a detach op, and only when the receiver installs its end do we toggle the committed bit. A minimal lifecycle helper might look like:\n```c\nint sc_track_transferable(struct sc_context *ctx, struct sc_transferable_desc *desc)\n{\n    switch (desc->flags & SC_KIND_MASK) {\n    case SC_KIND_ARRAYBUFFER:\n        memcpy(arena_alloc(ctx, desc->length), desc->handle, desc->length);\n        return sc_mark_detached(desc);\n    case SC_KIND_SHARED_ARRAYBUFFER:\n        return sc_register_sab_handle(ctx, (int)(intptr_t)desc->handle, desc->length);\n    case SC_KIND_MESSAGEPORT:\n        return sc_queue_port_move(ctx, desc);\n    default:\n        return -EINVAL;\n    }\n}\n```\nThese guardrails let workers exchange huge payloads without data races because the runtime records every ownership transition before the executor ever writes to an IPC queue.",
      "is_complete": false,
      "next_focus": "Worked example: step-by-step cloning of a nested object graph containing TypedArrays, highlighting stack operations and buffer duplication."
    },
    {
      "schema_version": "lesson_section_v1",
      "role": "lesson-expand",
      "section_id": "worked-example-typedarray-clone",
      "type": "worked_example",
      "content": "To understand typed array cloning under load, imagine the JS payload `message = { matrix: new Float32Array(6), channels: [ { mask: new Uint8Array(4) }, new Uint8Array(4) ] }`. The encoder starts by pushing the root object onto its DFS stack. Each time a container (object or array) is popped, it emits a tag plus child-count header, then pushes the children in reverse so the wire format preserves insertion order without recursion. When a TypedArray leaf appears, it reserves space in the clone buffer, copies the backing store bytes, records the byte length, and flips the source descriptor into the detached state.\n\nThe worked program below mirrors that control flow. Frames in `stack` model the traversal stack used inside `sc_encode_value`. `emit_typed` stands in for the ArrayBuffer duplication step: it writes the typed tag, the byte length, then the payload itself. Container frames write deterministic headers before descending; when `next_child` equals the child count, the frame is popped, recreating the unwind that would normally trigger reference-table bookkeeping.\n\nRunning it pushes three container frames (`message`, `channels`, `layer0`) and three leaves; `write` flushes the contiguous buffer to STDOUT, while `dprintf` reports the byte length. Because each container header is 8 bytes (tag + child count) and each TypedArray contributes `8 + payload_length` bytes, the reported `used` equals `3*8 + (8+24) + (8+4) + (8+4) = 80`, proving that every buffer was duplicated exactly once before the traversal stack empties.\n\n```c\n#include <stdint.h>\n#include <stddef.h>\n#include <stdlib.h>\n#include <string.h>\n#include <stdio.h>\n#include <unistd.h>\n\nenum node_kind {\n    NODE_OBJECT = 0x10u,\n    NODE_ARRAY = 0x11u,\n    NODE_TYPED_FLOAT32 = 0x20u,\n    NODE_TYPED_UINT8 = 0x21u\n};\n\nstruct typed_view {\n    const void *data;\n    size_t length;\n    uint32_t tag;\n};\n\nstruct node {\n    enum node_kind kind;\n    const char *label;\n    union {\n        struct {\n            const struct node *const *children;\n            size_t child_count;\n        } comp;\n        struct typed_view typed;\n    } as;\n};\n\nstruct clone_frame {\n    const struct node *node;\n    size_t next_child;\n};\n\nstatic size_t emit_typed(uint8_t *dst, size_t cap, const struct typed_view *view)\n{\n    const size_t needed = sizeof(uint32_t) * 2 + view->length;\n    if (cap < needed)\n        return 0;\n    memcpy(dst, &view->tag, sizeof(uint32_t));\n    uint32_t bytes = (uint32_t)view->length;\n    memcpy(dst + sizeof(uint32_t), &bytes, sizeof(uint32_t));\n    memcpy(dst + sizeof(uint32_t) * 2, view->data, view->length);\n    return needed;\n}\n\nint main(void)\n{\n    float matrix_data[6] = { 1.f, 0.f, 0.f, 0.f, 1.f, 0.f };\n    uint8_t mask_data[4] = { 0, 1, 1, 0 };\n    uint8_t gradient_data[4] = { 4, 5, 6, 7 };\n\n    struct typed_view matrix_view = { matrix_data, sizeof(matrix_data), NODE_TYPED_FLOAT32 };\n    struct typed_view mask_view = { mask_data, sizeof(mask_data), NODE_TYPED_UINT8 };\n    struct typed_view gradient_view = { gradient_data, sizeof(gradient_data), NODE_TYPED_UINT8 };\n\n    struct node matrix_node = { NODE_TYPED_FLOAT32, \"matrix\", .as.typed = matrix_view };\n    struct node mask_node = { NODE_TYPED_UINT8, \"mask\", .as.typed = mask_view };\n    struct node gradient_node = { NODE_TYPED_UINT8, \"gradient\", .as.typed = gradient_view };\n\n    const struct node *const layer_children[] = { &mask_node };\n    struct node layer_obj = { NODE_OBJECT, \"layer0\", .as.comp = { layer_children, 1 } };\n\n    const struct node *const channel_children[] = { &layer_obj, &gradient_node };\n    struct node channels = { NODE_ARRAY, \"channels\", .as.comp = { channel_children, 2 } };\n\n    const struct node *const root_children[] = { &matrix_node, &channels };\n    struct node root = { NODE_OBJECT, \"message\", .as.comp = { root_children, 2 } };\n\n    struct clone_frame stack[16];\n    size_t sp = 0;\n    stack[sp++] = (struct clone_frame){ .node = &root, .next_child = 0 };\n\n    uint8_t buffer[512];\n    size_t used = 0;\n\n    while (sp > 0) {\n        struct clone_frame *frame = &stack[sp - 1];\n        if (frame->node->kind == NODE_TYPED_FLOAT32 || frame->node->kind == NODE_TYPED_UINT8) {\n            size_t wrote = emit_typed(buffer + used, sizeof(buffer) - used, &frame->node->as.typed);\n            if (!wrote)\n                return EXIT_FAILURE;\n            used += wrote;\n            --sp;\n            continue;\n        }\n\n        if (frame->next_child == 0) {\n            uint32_t header[2] = {\n                (uint32_t)frame->node->kind,\n                (uint32_t)frame->node->as.comp.child_count\n            };\n            if (sizeof(buffer) - used < sizeof(header))\n                return EXIT_FAILURE;\n            memcpy(buffer + used, header, sizeof(header));\n            used += sizeof(header);\n        }\n\n        if (frame->next_child == frame->node->as.comp.child_count) {\n            --sp;\n            continue;\n        }\n\n        if (sp == sizeof(stack) / sizeof(stack[0]))\n            return EXIT_FAILURE;\n\n        const struct node *child = frame->node->as.comp.children[frame->next_child++];\n        stack[sp++] = (struct clone_frame){ .node = child, .next_child = 0 };\n    }\n\n    if (write(STDOUT_FILENO, buffer, used) < 0)\n        return EXIT_FAILURE;\n\n    dprintf(STDERR_FILENO, \"serialized %zu bytes\\n\", used);\n    return EXIT_SUCCESS;\n}\n```\n",
      "is_complete": false,
      "next_focus": "Worked example: transferring a MessagePort plus SharedArrayBuffer between workers, detailing the syscall/IPC hooks and state transitions."
    },
    {
      "schema_version": "lesson_section_v1",
      "role": "lesson-expand",
      "section_id": "worked-example-port-sab-transfer",
      "type": "worked_example",
      "content": "Moving a MessagePort and a SharedArrayBuffer in one transaction means the executor must juggle both ownership tables and OS handles. First, the encoder validates that the SAB was created with atomics (otherwise multiple workers could observe torn writes). Next it locks both port endpoints, marks the sender half as \"detaching\", and stages the SAB handle into the transfer list without copying any bytes. Once the payload bytes are queued, the executor emits an eventfd tick to wake the destination worker; only when the worker acks does the runtime flip both assets to the \"committed\" state.\n```c\n#include <errno.h>\n#include <stdint.h>\n#include <stdio.h>\n#include <string.h>\n#include <sys/eventfd.h>\n#include <unistd.h>\n\nstruct sc_transferable_desc { void *handle; size_t length; unsigned flags; };\nstruct sc_message_port { int queue_fd; int signal_fd; int state; };\n\nstatic int move_port(struct sc_message_port *port)\n{\n    if (port->state != 0)\n        return -EBUSY;\n    port->state = 1; /* detaching */\n    uint64_t tick = 1;\n    if (write(port->signal_fd, &tick, sizeof(tick)) != sizeof(tick))\n        return -EIO;\n    return 0;\n}\n\nint main(void)\n{\n    int queue_pipe[2];\n    pipe(queue_pipe);\n    int signal_fd = eventfd(0, 0);\n    struct sc_message_port port = { queue_pipe[1], signal_fd, 0 };\n\n    struct sc_transferable_desc sab = {\n        .handle = (void *)(intptr_t)42, /* shared memory handle */\n        .length = 4096,\n        .flags = 0x1u /* SAB_FLAG_HAS_ATOMICS */\n    };\n\n    if (move_port(&port) < 0)\n        return 1;\n\n    const char msg[] = \"postMessage payload\";\n    write(port.queue_fd, msg, sizeof(msg));\n\n    uint64_t ack;\n    read(signal_fd, &ack, sizeof(ack));\n    port.state = 2;      /* committed */\n    sab.flags |= 0x2u;   /* mark transferred */\n\n    dprintf(STDOUT_FILENO, \"queued %zu bytes, handle=%p committed\\n\",\n            sizeof(msg), sab.handle);\n    return 0;\n}\n```\nHere the sender moves the port by writing to its signal fd before the data hits `queue_fd`, mirroring how browsers gate MessagePort ownership. The SAB never copies; instead the descriptor’s `handle` is tagged as transferred so the receiver can map it with `mmap` and increment the shared refcount once its worker loop sees the wakeup tick.",
      "is_complete": false,
      "next_focus": "Break down the serialized format layout (tags, lengths, handles) and differences for SharedArrayBuffer versus regular buffers."
    }
  ],
  "next_focus": "Break down the serialized format layout (tags, lengths, handles) and differences for SharedArrayBuffer versus regular buffers."
}